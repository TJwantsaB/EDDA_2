---
title: "Assignment 2"
author: "Lukas Unruh, Teije Langelaan, Gidon Quint"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(333)
library(lme4)
library(Matrix)
library(carData)
options(digits=3)
```

# Exercise 1

```{r}
fruitfly_data=read.table(file="fruitflies.txt",header=TRUE)
```

## Lukas)

### a)

### b)

### c)

### d)

### e)

## Gid)

### a)

### b)

### c)

### d)

### e)

## TJ)

### a)

### b)

### c)

### d)

### e)

# Exercise 2

```{r}
birthweight_data=read.csv(file="Birthweight.csv",header=TRUE)

```
In our data set we have multiple continuous and discrete variables. However, for the model we do not have to state that discrete variables are factors as all of them only have 2 levels (no = 0 and yes = 1).
## Lukas)

### a)

### b)

### c)

### d)

### e)

### f)

### g)

### h)

### i)

## Gid)

### a)

### b)

### c)

### d)

### e)

### f)

### g)

### h)

### i)

## TJ)
### a)

```{r}
# Assuming birthweight_data is your dataset
summary(birthweight_data)

# Fit a linear model
TJ_BW <- lm(Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mheight + mppwt + fage + fedyrs + fnocig + fheight, data=birthweight_data)

# Plot diagnostics for influential points
par(mfrow=c(2,2))
plot(TJ_BW, which=1:4) # Plots for residuals, leverage, etc.
round(cooks.distance(TJ_BW),2)


```

Upon inspection of Cook's distances, we conclude that there are no influence points in the data set, as no observation has a Cook's distance greater than 1.

```{r}
predictors <- birthweight_data[, c("Length", "Headcirc", "Gestation", "mage", "mnocig", "mheight", "mppwt", "fage", "fedyrs", "fnocig", "fheight")]
print(cor(predictors))
library(carData)
vif_values <- vif(TJ_BW)
print(vif_values)
```

Upon inspection of correlation matrix, we note that variables mage and fage appear to be highly correlated (0.806), however their VIF values are under 5, therefore we conclude that the model has no problem of collinearity.

### b)
```{r}
summary(TJ_BW)
TJ_BW_reduced <- step(TJ_BW, direction = "backward")
summary(TJ_BW_reduced)
```
Using the step-down approach we arrived at the following model: Birthweight ~ 
Length + Headcirc + Gestation + mage + mppwt + fheight with R-squared: 0.757

```{r}
plot(TJ_BW_reduced, which = 1)

qqnorm(residuals(TJ_BW_reduced))
qqline(residuals(TJ_BW_reduced))
shapiro.test(residuals(TJ_BW_reduced))
```
In the residuals vs fitted plot, the pattern from 3.0 to 3.5 could hint at non-linearity, adding polynomial or interaction terms might improve fit. Upon further inspection of the QQ-plot and shapiro-Wilk test we conclude that the assumption of normally distributed residuals is not met.

### c)
```{r}
# Calculate the average values for the predictors
avg_data <- data.frame(
  Length = mean(birthweight_data$Length, na.rm = TRUE),
  Headcirc = mean(birthweight_data$Headcirc, na.rm = TRUE),
  Gestation = mean(birthweight_data$Gestation, na.rm = TRUE),
  mage = mean(birthweight_data$mage, na.rm = TRUE),
  mppwt = mean(birthweight_data$mppwt, na.rm = TRUE),
  fheight = mean(birthweight_data$fheight, na.rm = TRUE)
)

# Assuming your model is named 'TJ_BW_reduced' and is already fitted with the correct formula
# Predict the Birthweight for the average values with 95% confidence interval
confidence_interval <- predict(TJ_BW_reduced, newdata = avg_data, interval = "confidence", level = 0.95)

# Predict the Birthweight for the average values with 95% prediction interval
prediction_interval <- predict(TJ_BW_reduced, newdata = avg_data, interval = "prediction", level = 0.95)

# View the results
confidence_interval
prediction_interval
```

### d)
```{r}
library(glmnet)
set.seed(3)
x=as.matrix(birthweight_data[,-3]) #remove the response variable
y=as.double(as.matrix(birthweight_data[,3])) #only the response variable
train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train] # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality
lasso.mod=glmnet(x.train,y.train,alpha=1)
cv.lasso=cv.glmnet(x.train,y.train,alpha=1,type.measure="mse")
plot(lasso.mod,label=T,xvar="lambda") #have a look at the lasso path
plot(cv.lasso) # the best lambda by cross-validation
lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
coef(lasso.mod,s=cv.lasso$lambda.1se) #betaâ€™s for the best lambda
y.pred=predict(lasso.mod,s=lambda.1se,newx=x.test) #predict for test
mse.lasso=mean((y.test-y.pred)^2) #mse for the predicted test rows

TJ_lasso <- lm(Birthweight ~ Length +  Gestation + mage + mnocig + mheight + mppwt + fage + fedyrs + fnocig + fheight, data=birthweight_data)
```
Lasso model: Birthweight ~ Length + Headcirc + Gestation + lowbwt
Step down:   Birthweight ~ Length + Headcirc + Gestation + mage + mppwt + fheight

The Lasso model and the step-down method both identify Length, Head Circumference, and Gestation as significant predictors of Birthweight, indicating their critical role. The Lasso model is more parsemonious, using two fewer variables. It uniquely includes lowbwt, a binary variable distinguishing between birth weights below and above 6 pounds, which the step-down model omits. Conversely, the step-down model retains mage, mppwt, and fheight, suggesting these factors may have some predictive value not captured by the Lasso model, likely due to its penalty on model complexity that excludes less impactful variables.
### e)

### f)

### g)

### h)

### i)

# Exercise 3

```{r}
awards_data=read.table(file="awards.txt",header=TRUE)
```

## Lukas)

### a)

### b)

### c)

## Gid)

### a)

### b)

### c)

## TJ)

### a)

### b)

### c)
